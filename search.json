[
  {
    "objectID": "src/EDA - Phoebe Pan.html",
    "href": "src/EDA - Phoebe Pan.html",
    "title": "\n2  Exploratory Data Analysis: Phoebe Pan\n",
    "section": "",
    "text": "2.1 Introduction\nThis document contains exploratory data analysis (EDA) for the group project, focusing on stock market volatility patterns and predictors across major global markets from 2000 to 2025.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis: Phoebe Pan</span>"
    ]
  },
  {
    "objectID": "src/EDA - Phoebe Pan.html#data-import",
    "href": "src/EDA - Phoebe Pan.html#data-import",
    "title": "\n2  Exploratory Data Analysis: Phoebe Pan\n",
    "section": "\n2.2 Data Import",
    "text": "2.2 Data Import\n\n# Load necessary packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyquant)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n── Attaching core tidyquant packages ─────────────────────── tidyquant 1.0.11 ──\n✔ PerformanceAnalytics 2.0.8      ✔ TTR                  0.24.4\n✔ quantmod             0.4.27     ✔ xts                  0.14.1── Conflicts ────────────────────────────────────────── tidyquant_conflicts() ──\n✖ zoo::as.Date()                 masks base::as.Date()\n✖ zoo::as.Date.numeric()         masks base::as.Date.numeric()\n✖ dplyr::filter()                masks stats::filter()\n✖ xts::first()                   masks dplyr::first()\n✖ dplyr::lag()                   masks stats::lag()\n✖ xts::last()                    masks dplyr::last()\n✖ PerformanceAnalytics::legend() masks graphics::legend()\n✖ quantmod::summary()            masks base::summary()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(TTR)\nlibrary(zoo)\n\n\n# import the data\nindices &lt;- c(\"^GSPC\", \"^FTSE\", \"^N225\", \"^GDAXI\", \"^FCHI\", \"^HSI\")\n\nmarket_data &lt;- tq_get(indices, get = \"stock.prices\",\n                      from = \"2000-01-01\",\n                      to = Sys.Date())\n\n\n# Glimpse of data structure\nglimpse(market_data)\n\nRows: 38,428\nColumns: 8\n$ symbol   &lt;chr&gt; \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\", \"^GSPC\"…\n$ date     &lt;date&gt; 2000-01-03, 2000-01-04, 2000-01-05, 2000-01-06, 2000-01-07, …\n$ open     &lt;dbl&gt; 1469.25, 1455.22, 1399.42, 1402.11, 1403.45, 1441.47, 1457.60…\n$ high     &lt;dbl&gt; 1478.00, 1455.22, 1413.27, 1411.90, 1441.47, 1464.36, 1458.66…\n$ low      &lt;dbl&gt; 1438.36, 1397.43, 1377.68, 1392.10, 1400.73, 1441.47, 1434.42…\n$ close    &lt;dbl&gt; 1455.22, 1399.42, 1402.11, 1403.45, 1441.47, 1457.60, 1438.56…\n$ volume   &lt;dbl&gt; 931800000, 1009000000, 1085500000, 1092300000, 1225200000, 10…\n$ adjusted &lt;dbl&gt; 1455.22, 1399.42, 1402.11, 1403.45, 1441.47, 1457.60, 1438.56…\n\n# Check missing values\nsapply(market_data, function(x) sum(is.na(x)))\n\n  symbol     date     open     high      low    close   volume adjusted \n       0        0      330      330      330      330      330      330 \n\n\nThis code provides a quick overview of the structure of the market_data dataset, confirming that we have data for multiple major indices with key variables such as open, close, high, low, volume, and adjusted price for each trading day from 2000 to the present.I examined the dataset for missing values and found that a small percentage (330 days) of entries are missing across several columns, including open, high, low, close, volume, and adjusted prices. The presence of missing values is typical for financial datasets due to market holidays or data reporting inconsistencies.\n\nggplot(market_data, aes(x = adjusted, fill = symbol)) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\") +\n  scale_x_log10() +\n  labs(title = \"Distribution of Adjusted Closing Prices\", x = \"Adjusted Close (log scale)\", y = \"Count\") +\n  facet_wrap(~ symbol, scales = \"free_y\") +\n  theme_minimal()\n\nWarning: Removed 330 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nThe histogram displays the distribution of adjusted closing prices for each major stock index, with prices shown on a logarithmic scale for better comparison in 25yrs. The log scale reveals the wide variation in price levels between different indices, such as the GSPC and Nikkei 225.\n\nggplot(market_data, aes(x = date, y = adjusted, color = symbol)) +\n  geom_line(alpha = 0.7) +\n  labs(title = \"Time Series of Adjusted Prices (2000–Present)\", x = \"Date\", y = \"Adjusted Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\nThe time series plot illustrates the trajectory of adjusted closing prices for all these indices from 2000 to 2025. Notably, the plot highlights the 2008 global financial crisis and the COVID-19 market shock, which appear as periods of sharp price declines or increased volatility across multiple indices.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis: Phoebe Pan</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html",
    "href": "src/EDA - Zhijun He.html",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "",
    "text": "3.1 Introduction\nIn this document, I present my exploratory data analysis (EDA) for our group project, focusing on stock market volatility patterns and predictors across major global markets from 2000 to 2025.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#data-import-and-preparation",
    "href": "src/EDA - Zhijun He.html#data-import-and-preparation",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.2 Data Import and Preparation",
    "text": "3.2 Data Import and Preparation\n\n# I'll first load all necessary packages for my analysis\nlibrary(tidyverse)  # For data manipulation and visualization\nlibrary(tidyquant)  # For accessing financial data\nlibrary(lubridate)  # For handling dates\nlibrary(TTR)        # For technical trading rules functions\nlibrary(zoo)        # For time series objects\n\n\n# I've selected these major global indices for my analysis\nindices &lt;- c(\n  \"^GSPC\" = \"S&P 500\",       # USA\n  \"^FTSE\" = \"FTSE 100\",      # UK\n  \"^N225\" = \"Nikkei 225\",    # Japan\n  \"^GDAXI\" = \"DAX\",          # Germany\n  \"^FCHI\" = \"CAC 40\",        # France\n  \"^HSI\" = \"Hang Seng\"       # Hong Kong\n)\n\n# I'm importing daily data for all indices from 2000 onwards\nmarket_data &lt;- tq_get(\n  names(indices),\n  get = \"stock.prices\",\n  from = \"2000-01-01\",\n  to = Sys.Date()\n) %&gt;%\n  # Adding descriptive names for better readability in my visualizations\n  mutate(index_name = indices[symbol])",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#data-exploration",
    "href": "src/EDA - Zhijun He.html#data-exploration",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.3 Data Exploration",
    "text": "3.3 Data Exploration\n\n# Let me examine the structure and quality of my dataset\ncat(\"My Dataset Overview:\\n\")\n\nMy Dataset Overview:\n\ncat(\"- Number of observations:\", nrow(market_data), \"\\n\")\n\n- Number of observations: 38428 \n\ncat(\"- Date range:\", min(market_data$date), \"to\", max(market_data$date), \"\\n\")\n\n- Date range: 10959 to 20206 \n\ncat(\"- Number of indices:\", length(unique(market_data$symbol)), \"\\n\\n\")\n\n- Number of indices: 6 \n\n# I'll check for missing values in my dataset\nmissing_values &lt;- market_data %&gt;%\n  summarize(across(everything(), ~sum(is.na(.)))) %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"Variable\", \n               values_to = \"Missing_Count\") %&gt;%\n  filter(Missing_Count &gt; 0)  # Only showing variables with missing values\n\nif(nrow(missing_values) &gt; 0) {\n  print(missing_values)\n} else {\n  cat(\"I found no missing values in my dataset.\\n\")\n}\n\n# A tibble: 6 × 2\n  Variable Missing_Count\n  &lt;chr&gt;            &lt;int&gt;\n1 open               330\n2 high               330\n3 low                330\n4 close              330\n5 volume             330\n6 adjusted           330\n\n# I'll take a quick look at the first few rows\nhead(market_data)\n\n# A tibble: 6 × 9\n  symbol date        open  high   low close     volume adjusted index_name\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;     \n1 ^GSPC  2000-01-03 1469. 1478  1438. 1455.  931800000    1455. S&P 500   \n2 ^GSPC  2000-01-04 1455. 1455. 1397. 1399. 1009000000    1399. S&P 500   \n3 ^GSPC  2000-01-05 1399. 1413. 1378. 1402. 1085500000    1402. S&P 500   \n4 ^GSPC  2000-01-06 1402. 1412. 1392. 1403. 1092300000    1403. S&P 500   \n5 ^GSPC  2000-01-07 1403. 1441. 1401. 1441. 1225200000    1441. S&P 500   \n6 ^GSPC  2000-01-10 1441. 1464. 1441. 1458. 1064800000    1458. S&P 500   \n\n\nMy dataset contains daily trading information for six major global indices from January 2000 onwards. I’ve collected date, opening, high, low, and closing prices, trading volume, and adjusted closing price for each index. This comprehensive dataset will allow me to analyze market behavior across different regions and economic conditions.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#volatility-analysis",
    "href": "src/EDA - Zhijun He.html#volatility-analysis",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.4 Volatility Analysis",
    "text": "3.4 Volatility Analysis\n\n# I'll calculate daily returns and volatility metrics for my analysis\nmarket_analysis &lt;- market_data %&gt;%\n  group_by(symbol, index_name) %&gt;%\n  arrange(date) %&gt;%\n  # Computing daily percentage returns\n  mutate(\n    daily_return = (adjusted / lag(adjusted) - 1) * 100,\n    # Adding a 20-day rolling volatility measure (annualized)\n    rolling_vol_20 = rollapply(\n      daily_return,\n      width = 20,\n      FUN = function(x) sd(x, na.rm = TRUE) * sqrt(252),\n      align = \"right\",\n      fill = NA\n    )\n  ) %&gt;%\n  ungroup()",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#visualization",
    "href": "src/EDA - Zhijun He.html#visualization",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.5 Visualization",
    "text": "3.5 Visualization\n\n# I'll visualize the long-term price trends for all indices\nggplot(market_data, aes(x = date, y = adjusted, color = index_name)) +\n  geom_line(linewidth = 0.7, alpha = 0.8) +\n  labs(\n    title = \"Adjusted Closing Prices of Major Global Indices (2000-Present)\",\n    subtitle = \"My comparison of long-term trends across different markets\",\n    x = \"Year\",\n    y = \"Adjusted Closing Price\",\n    color = \"Index\"\n  ) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\nIn my price trend analysis, I observe long-term upward trajectories for all six major global indices, though with varying magnitudes and volatility. I find that the S&P 500 demonstrates the most consistent growth pattern. In contrast, I notice that the Hang Seng and Nikkei 225 exhibit both higher price levels and greater volatility, particularly during the financial crises of 2008 and 2020. My analysis shows that while all indices suffered notable declines during these periods, each demonstrated its own distinctive recovery pace.\n\n# I'll normalize the volume data to make my comparisons more meaningful across indices\nnormalized_volume &lt;- market_data %&gt;%\n  group_by(symbol) %&gt;%\n  mutate(\n    norm_volume = volume / mean(volume, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n# Now I'll create my volume visualization\nggplot(normalized_volume, aes(x = date, y = norm_volume, color = index_name)) +\n  geom_line(alpha = 0.7) +\n  labs(\n    title = \"Normalized Trading Volume by Index (2000-Present)\",\n    subtitle = \"My analysis of volume patterns relative to mean levels\",\n    x = \"Year\",\n    y = \"Normalized Trading Volume\",\n    color = \"Index\"\n  ) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\nMy analysis of normalized trading volumes reveals significant insights across indices. I’ve normalized the data to enable fair comparisons between markets of different sizes. I can clearly identify peaks in trading activity during major market events like the 2008 financial crisis and the 2020 pandemic. In my observations, the S&P 500 and Hang Seng Index demonstrate the most volatile trading patterns, particularly after 2010. I believe this increased volatility likely stems from growing global participation in the US and Chinese markets.\n\n# I'll visualize the volatility patterns I've calculated\nmarket_analysis %&gt;%\n  filter(!is.na(rolling_vol_20)) %&gt;%\n  ggplot(aes(x = date, y = rolling_vol_20, color = index_name)) +\n  geom_line(alpha = 0.8) +\n  labs(\n    title = \"20-Day Rolling Volatility of Major Indices (Annualized)\",\n    subtitle = \"My analysis of market uncertainty over time\",\n    x = \"Year\",\n    y = \"Annualized Volatility (%)\",\n    color = \"Index\"\n  ) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 0.01)) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#market-correlation-analysis",
    "href": "src/EDA - Zhijun He.html#market-correlation-analysis",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.6 Market Correlation Analysis",
    "text": "3.6 Market Correlation Analysis\n\n# I want to understand how these markets move together\n# First I'll prepare daily returns by index\ndaily_returns &lt;- market_analysis %&gt;%\n  select(date, symbol, daily_return) %&gt;%\n  filter(!is.na(daily_return)) %&gt;%\n  pivot_wider(\n    names_from = symbol,\n    values_from = daily_return\n  )\n\n# Now I'll calculate my correlation matrix\ncor_matrix &lt;- cor(\n  daily_returns %&gt;% select(-date), \n  use = \"pairwise.complete.obs\"\n)\n\n# Converting to long format for my visualization\ncor_data &lt;- cor_matrix %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Index1\") %&gt;%\n  pivot_longer(\n    cols = -Index1,\n    names_to = \"Index2\",\n    values_to = \"Correlation\"\n  )\n\n# Mapping index symbols to names for better readability in my plot\ncor_data &lt;- cor_data %&gt;%\n  mutate(\n    Index1_Name = indices[Index1],\n    Index2_Name = indices[Index2]\n  )\n\n# Creating my correlation heatmap\nggplot(cor_data, aes(x = Index1_Name, y = Index2_Name, fill = Correlation)) +\n  geom_tile() +\n  geom_text(aes(label = round(Correlation, 2)), color = \"white\", size = 4) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",\n    midpoint = 0.5, limits = c(0, 1)\n  ) +\n  labs(\n    title = \"My Correlation Analysis of Daily Returns (2000-Present)\",\n    subtitle = \"Higher values indicate stronger co-movement between markets\",\n    x = NULL, y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#seasonal-patterns",
    "href": "src/EDA - Zhijun He.html#seasonal-patterns",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.7 Seasonal Patterns",
    "text": "3.7 Seasonal Patterns\n\n# I'm interested in identifying seasonal patterns in returns\nseasonal_data &lt;- market_analysis %&gt;%\n  mutate(\n    month = month(date, label = TRUE),\n    year = year(date)\n  ) %&gt;%\n  filter(!is.na(daily_return))\n\n# I'll calculate average monthly returns by index\nmonthly_returns &lt;- seasonal_data %&gt;%\n  group_by(index_name, month) %&gt;%\n  summarize(\n    avg_return = mean(daily_return, na.rm = TRUE),\n    volatility = sd(daily_return, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Now I'll visualize my monthly return patterns\nggplot(monthly_returns, aes(x = month, y = avg_return, fill = index_name)) +\n  geom_col(position = \"dodge\") +\n  geom_errorbar(\n    aes(ymin = avg_return - volatility/sqrt(20), \n        ymax = avg_return + volatility/sqrt(20)),\n    position = position_dodge(0.9),\n    width = 0.2\n  ) +\n  labs(\n    title = \"My Analysis of Average Monthly Returns by Index\",\n    subtitle = \"With standard error bars to show confidence in my findings\",\n    x = \"Month\",\n    y = \"Average Daily Return (%)\",\n    fill = \"Index\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\")\n  )",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#summary-statistics",
    "href": "src/EDA - Zhijun He.html#summary-statistics",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.8 Summary Statistics",
    "text": "3.8 Summary Statistics\n\n# I'll generate comprehensive summary statistics for my report\nmarket_summary &lt;- market_analysis %&gt;%\n  group_by(index_name) %&gt;%\n  summarize(\n    avg_daily_return = mean(daily_return, na.rm = TRUE),\n    median_daily_return = median(daily_return, na.rm = TRUE),\n    volatility = sd(daily_return, na.rm = TRUE),\n    min_return = min(daily_return, na.rm = TRUE),\n    max_return = max(daily_return, na.rm = TRUE),\n    sharpe_ratio = mean(daily_return, na.rm = TRUE) / sd(daily_return, na.rm = TRUE),\n    data_completeness = 1 - (sum(is.na(daily_return)) / n())\n  ) %&gt;%\n  arrange(desc(sharpe_ratio))\n\n# I'll display my summary statistics in a clean table\nmarket_summary %&gt;%\n  knitr::kable(\n    digits = 4,\n    caption = \"My Summary Statistics by Index (2000-Present)\"\n  )\n\n\nMy Summary Statistics by Index (2000-Present)\n\n\n\n\n\n\n\n\n\n\n\nindex_name\navg_daily_return\nmedian_daily_return\nvolatility\nmin_return\nmax_return\nsharpe_ratio\ndata_completeness\n\n\n\nS&P 500\n0.0285\n0.0606\n1.2304\n-11.9841\n11.5800\n0.0232\n0.9998\n\n\nDAX\n0.0271\n0.0766\n1.4249\n-12.2386\n11.4020\n0.0190\n0.9890\n\n\nNikkei 225\n0.0143\n0.0468\n1.4588\n-12.3958\n14.1503\n0.0098\n0.9652\n\n\nCAC 40\n0.0119\n0.0448\n1.3846\n-12.2768\n11.1762\n0.0086\n0.9905\n\n\nFTSE 100\n0.0088\n0.0458\n1.1442\n-10.8738\n9.8387\n0.0077\n0.9863\n\n\nHang Seng\n0.0114\n0.0290\n1.4904\n-13.2233\n14.3471\n0.0077\n0.9777",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/EDA - Zhijun He.html#conclusion",
    "href": "src/EDA - Zhijun He.html#conclusion",
    "title": "\n3  Exploratory Data Analysis: Zhijun He‘s EDA2\n",
    "section": "\n3.9 Conclusion",
    "text": "3.9 Conclusion\nThrough my exploratory analysis, I’ve uncovered distinct patterns in volatility, returns, and trading activity across major global stock indices. My optimized visualizations highlight market correlations, seasonal patterns, and the varying impacts of major financial events across different markets. These insights from my analysis provide a foundation for our group’s further investigation of volatility predictors and potential trading strategies. As I progress with this project, I plan to explore more advanced volatility modeling techniques and examine how macroeconomic factors influence these patterns across different regions.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis: Zhijun He‘s EDA2</span>"
    ]
  },
  {
    "objectID": "src/appx/effective teamwork - Zhijun.html",
    "href": "src/appx/effective teamwork - Zhijun.html",
    "title": "4  Building Skills for Effective Teamwork-Team Starfall Reading",
    "section": "",
    "text": "4.1 Summary of “Teamwork Is Hard Work”\nIn the Forbes article “Teamwork Is Hard Work. Here’s How To Build The Skills To Do It Well” by Ann Kowal Smith, the author explores the crucial skills needed for effective collaboration in today’s workplace. Smith argues that despite human beings’ social nature, our educational and professional systems often reward individual performance rather than collaborative efforts. This creates a contradiction as modern work increasingly requires teamwork, with collaborative work having increased by over 50% in the past two decades.\nThe article presents five specific skills that build what researchers call “collective intelligence” - an IQ-like factor that determines how well teams solve challenges together. Unlike individual intelligence, collective intelligence depends on how team members engage with each other rather than individual brilliance.",
    "crumbs": [
      "Teamwork",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building Skills for Effective Teamwork-Team Starfall Reading</span>"
    ]
  },
  {
    "objectID": "src/appx/effective teamwork - Zhijun.html#summary-of-teamwork-is-hard-work",
    "href": "src/appx/effective teamwork - Zhijun.html#summary-of-teamwork-is-hard-work",
    "title": "4  Building Skills for Effective Teamwork-Team Starfall Reading",
    "section": "",
    "text": "4.1.1 The Five Key Skills for Collective Intelligence\n\nListening with humility: Active, engaged listening is described as a “productivity power tool” that enhances innovation and enables collaboration. The article notes that humans typically overestimate their listening abilities, and real listening requires training the brain to focus despite distractions.\nAsking good and curious questions: Open-ended questions that explore possibilities (“what if” instead of “what”) create space for learning something new or unexpected. Good questions signal respect for others’ contributions and help surface valuable insights.\nChallenging strongly held assumptions: Expertise can lead to inflexibility and confirmation bias. By learning to temporarily suspend our beliefs and challenge our assumptions, we can open our minds to different perspectives.\nDisagreeing with respect and without retribution: Healthy disagreement serves as a check on overconfidence and groupthink, but requires psychological safety - the belief that team members can raise difficult issues without negative consequences.\nWidening the circle of empathy: Our tendency to divide the world into “us” and “them” creates biases that disrupt collaboration. With practice, we can extend our empathy beyond our immediate group, building deeper trust with diverse colleagues.\n\nThe article concludes that these skills aren’t innate but require deliberate practice - just as we would never expect a sports team to perform without practicing together. By developing these collaborative capabilities, teams can effectively address complex challenges that individuals simply cannot solve alone.",
    "crumbs": [
      "Teamwork",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building Skills for Effective Teamwork-Team Starfall Reading</span>"
    ]
  },
  {
    "objectID": "src/appx/effective teamwork - Zhijun.html#critical-analysis-and-discussion",
    "href": "src/appx/effective teamwork - Zhijun.html#critical-analysis-and-discussion",
    "title": "4  Building Skills for Effective Teamwork-Team Starfall Reading",
    "section": "4.2 Critical Analysis and Discussion",
    "text": "4.2 Critical Analysis and Discussion\nSmith’s framework for collective intelligence provides valuable insights, but requires deeper examination to understand its practical implications and potential limitations in diverse organizational contexts.\n\n4.2.1 The Gap Between Theory and Practice\nWhile the five skills identified by Smith are compelling in theory, implementing them within existing organizational structures presents significant challenges. Most organizations continue to evaluate performance using metrics that prioritize individual achievement over collaborative success. As Smith notes, companies “hire, promote and reward individuals, hoping that good teams will just somehow happen” - a fundamental contradiction that undermines collective efforts.\nThis systemic misalignment raises critical questions: How can organizations genuinely foster collaborative environments when their reward structures incentivize competition? What structural changes are necessary to bridge this gap? Successful implementation likely requires reimagining performance evaluation systems to explicitly recognize collaborative contributions alongside individual achievements.\n\n\n4.2.2 Power Dynamics and Psychological Safety\nThe article’s emphasis on psychological safety as foundational to effective teamwork aligns with broader research on high-performing teams. However, Smith’s discussion could benefit from deeper exploration of how power imbalances affect psychological safety. In hierarchical organizations, the ability to “disagree with respect and without retribution” is significantly constrained by organizational position.\nThis is powerfully illustrated in the article’s alarming statistic that “90% of nurses would not contradict a doctor, even if a patient’s life is at risk.” This example reveals how deeply entrenched power dynamics can override even life-or-death considerations. Organizations serious about fostering collective intelligence must acknowledge and address these power imbalances through both cultural and structural interventions.\n\n\n4.2.3 The Neurological Basis for Team Performance\nSmith briefly references the neurological basis for our social nature and biases, noting how we are “hard-wired to break the world into Us and Them.” This neurological perspective merits further exploration. Recent advances in social neuroscience demonstrate that effective collaboration activates neural networks associated with trust and reward, while perceived threats to status or autonomy trigger defensive responses that inhibit creative thinking and collaboration.\nUnderstanding these neurological underpinnings could help teams develop more effective strategies for managing conflict and building trust. Organizations might benefit from incorporating insights from neuroscience into team development programs, focusing on creating environments that neurologically support rather than inhibit collaboration.\n\n\n4.2.4 Balancing Individual Excellence and Team Performance\nA critical tension exists between developing individual expertise and fostering collective intelligence. Smith references the Dunning-Kruger effect and notes that “a lot of learning is dangerous too” because “expertise entrenches us.” This presents a paradox: organizations need both deep expertise and collaborative flexibility.\nFinding this balance requires careful consideration of how expertise is developed and deployed within teams. Potential approaches include:\n\nStructuring teams to include both specialists and generalists\nRotating leadership based on relevant expertise for specific challenges\nImplementing processes that deliberately challenge expert assumptions\nCreating psychological safety that allows experts to acknowledge uncertainty\n\n\n\n4.2.5 Measuring and Developing Collective Intelligence\nAlthough Smith establishes the importance of collective intelligence, the article provides limited guidance on how to measure or systematically develop it. This presents an opportunity for organizations to create assessment frameworks that evaluate team dynamics alongside outcomes.\nEffective development of these skills likely requires deliberate practice opportunities, regular feedback, and organizational support. Just as individual skill development requires dedicated time and resources, building collective intelligence demands institutional commitment to collaborative learning.",
    "crumbs": [
      "Teamwork",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building Skills for Effective Teamwork-Team Starfall Reading</span>"
    ]
  },
  {
    "objectID": "src/appx/effective teamwork - Zhijun.html#personal-reflection-and-application",
    "href": "src/appx/effective teamwork - Zhijun.html#personal-reflection-and-application",
    "title": "4  Building Skills for Effective Teamwork-Team Starfall Reading",
    "section": "4.3 Personal Reflection and Application",
    "text": "4.3 Personal Reflection and Application\nThis article resonates with my own experiences working in teams. The emphasis on psychological safety particularly stands out as a foundation for the other skills. When team members feel safe to express ideas without fear of judgment or retribution, creativity and problem-solving flourish.I’ve observed that in teams where members practice active listening and ask curious questions, the quality of solutions improves dramatically. These skills create an environment where diverse perspectives can be shared and integrated into better outcomes.\nThe challenge of “widening the circle of empathy” seems especially relevant in today’s diverse workplaces. As teams become more global and cross-functional, the ability to understand and appreciate different viewpoints becomes increasingly valuable. Moving forward, I plan to be more intentional about practicing these five skills in my collaborative work, particularly focusing on challenging my own assumptions and creating space for respectful disagreement. I also intend to advocate for structural changes that better align reward systems with collaborative goals within my organization.",
    "crumbs": [
      "Teamwork",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building Skills for Effective Teamwork-Team Starfall Reading</span>"
    ]
  },
  {
    "objectID": "src/appx/effective teamwork - Phoebe.html",
    "href": "src/appx/effective teamwork - Phoebe.html",
    "title": "5  Effective teamwork",
    "section": "",
    "text": "In this article “What Google Learned From Its Quest to Build the Perfect Team”, Charles Duhigg explores how Google launched an internal research initiative called Project Aristotle to investigate what makes a team successful. The company studied more than 180 teams to determine why some performed well while others did not, even when team members had similar talent and access to resources.\nGoogle’s research revealed that psychological safety was the most important ingredient for team success. When members felt safe to speak up, take risks, and admit mistakes without fear, teams performed better. In addition, four other factors played key roles: dependability, structure and clarity, meaning, and impact.\nInterestingly, the team composition mattered less than the collaboration. Teams that communicated openly and respected each other outperformed even the most technically skilled groups. In short, trust and connection beat talent alone.",
    "crumbs": [
      "Teamwork",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Effective teamwork</span>"
    ]
  },
  {
    "objectID": "src/appx/proposal.html",
    "href": "src/appx/proposal.html",
    "title": "\n6  Proposal\n",
    "section": "",
    "text": "6.1 Team Members",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "src/appx/proposal.html#team-members",
    "href": "src/appx/proposal.html#team-members",
    "title": "\n6  Proposal\n",
    "section": "",
    "text": "Zhijun He\nPhoebe Pan",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "src/appx/proposal.html#project-description",
    "href": "src/appx/proposal.html#project-description",
    "title": "\n6  Proposal\n",
    "section": "\n6.2 Project Description",
    "text": "6.2 Project Description\nOur project will analyze historical stock market fluctuations from 2000-2025, examining how major economic, political, and global events impact market volatility across different sectors. We’ll collect and analyze comprehensive market data to identify patterns that precede significant market movements and evaluate the effectiveness of various prediction models in anticipating market corrections and rallies.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "src/appx/proposal.html#motivation-and-inspiration",
    "href": "src/appx/proposal.html#motivation-and-inspiration",
    "title": "\n6  Proposal\n",
    "section": "\n6.3 Motivation and Inspiration",
    "text": "6.3 Motivation and Inspiration\nOur team selected this project due to its practical relevance in the financial world. Understanding market volatility has direct applications for investment strategies and risk management, which can benefit both individual investors and financial institutions. The abundance of historical market data spanning multiple market cycles, including several major crashes and recoveries, provides us with rich material for analysis and pattern recognition.\nWe are particularly drawn to the interdisciplinary nature of this project, which allows us to combine financial knowledge, statistical analysis, and machine learning techniques. Recent market fluctuations due to global events have only heightened our interest, as they provide timely case studies for our analysis. Both Zhijun and Phoebe share a strong interest in financial markets and quantitative analysis, making this project a natural fit for our skills and academic goals.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "src/appx/proposal.html#implementation-plan",
    "href": "src/appx/proposal.html#implementation-plan",
    "title": "\n6  Proposal\n",
    "section": "\n6.4 Implementation Plan",
    "text": "6.4 Implementation Plan\nThis accelerated 2-week plan begins on April 9, 2025 and concludes on April 23, 2025.\n\n\n\nProject Timeline and Responsibilities\n\n\n\n\n\n\n\nTask\nDeliverable\nTeam.Member\nTimeline\n\n\n\nData Collection\nCurated dataset of market indices, sector ETFs, and volatility metrics\nZhijun He\nApril 9-11\n\n\nHistorical Event Mapping\nTimeline of major events correlated with market movements\nPhoebe Pan\nApril 9-11\n\n\nExploratory Data Analysis\nInitial visualizations and statistical summaries\nZhijun He\nApril 12-14\n\n\nPattern Identification\nReport on identified volatility patterns\nBoth members\nApril 14-16\n\n\nModel Development\nImplementation of prediction models\nBoth members\nApril 15-18\n\n\nModel Evaluation\nPerformance metrics and comparative analysis\nPhoebe Pan\nApril 18-20\n\n\nVisualization Dashboard\nInteractive web interface for exploring findings\nZhijun He\nApril 18-21\n\n\nDocumentation\nProject report, code documentation, and user guide\nBoth members\nApril 19-22\n\n\nFinal Presentation\nSlides and demonstration\nBoth members\nApril 23",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "src/appx/proposal.html#expected-outcomes",
    "href": "src/appx/proposal.html#expected-outcomes",
    "title": "\n6  Proposal\n",
    "section": "\n6.5 Expected Outcomes",
    "text": "6.5 Expected Outcomes\nUpon completion of this two-week project, we expect to deliver:\n\nA comprehensive analysis of market volatility patterns from 2000-2025\nIdentification of leading indicators for market shifts\nEvaluation of prediction model performance across different market conditions\nInteractive visualization dashboard for exploring market volatility patterns\nRecommendations for investors based on historical pattern analysis",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "src/appx/case-study.html",
    "href": "src/appx/case-study.html",
    "title": "7  Case Study:What Have We Learned from the German Story?",
    "section": "",
    "text": "8 The art of making data speak: Analyzing structures and techniques in modern data journalism\nData journalism has evolved from simple chart supplements to become a sophisticated storytelling medium that makes complex information accessible, engaging, and meaningful. By examining how data stories are structured and analyzing best practices in data visualization integration, this research illuminates the frameworks and techniques that underpin effective data storytelling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study:What Have We Learned from the German Story?</span>"
    ]
  },
  {
    "objectID": "src/appx/case-study.html#the-architecture-of-data-narratives-how-stories-unfold-through-numbers",
    "href": "src/appx/case-study.html#the-architecture-of-data-narratives-how-stories-unfold-through-numbers",
    "title": "7  Case Study:What Have We Learned from the German Story?",
    "section": "8.1 The architecture of data narratives: How stories unfold through numbers",
    "text": "8.1 The architecture of data narratives: How stories unfold through numbers\nData stories typically follow distinct organizational patterns that guide readers from curiosity to understanding. Three dominant structural frameworks have emerged as particularly effective:\nThe martini glass structure, developed by Segel and Heer, begins with a narrow, author-directed narrative (the stem) before widening to allow audience exploration (the glass). This structure initially guides readers through key insights before encouraging independent investigation. It’s particularly effective for introducing unfamiliar topics to general audiences, as it provides necessary context before inviting exploration.\nThe inverted pyramid approach frontloads key findings and supporting evidence before ending with methodology or background context. This structure—borrowed from traditional journalism—allows readers to quickly grasp main points even with partial reading.\nThe drill-down structure presents a high-level overview first, then enables readers to explore specific areas of interest in greater detail. This approach balances guidance with reader agency and works well for complex topics with multiple dimensions.\nThe beginning of effective data stories typically establishes context, presents a clear thesis, introduces the dataset, and offers a compelling hook—often a surprising statistic or provocative question. The development progresses through a logical sequence of insights supported by visualizations, while the conclusion crystallizes the central insight and connects findings to real-world implications.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study:What Have We Learned from the German Story?</span>"
    ]
  },
  {
    "objectID": "src/appx/case-study.html#visual-textual-integration-when-charts-become-narrative-elements",
    "href": "src/appx/case-study.html#visual-textual-integration-when-charts-become-narrative-elements",
    "title": "7  Case Study:What Have We Learned from the German Story?",
    "section": "8.2 Visual-textual integration: When charts become narrative elements",
    "text": "8.2 Visual-textual integration: When charts become narrative elements\nUnlike decorative illustrations, visualizations in effective data journalism function as integral narrative components. Research reveals several integration approaches that enhance understanding:\nScrollytelling—where visualizations transform as readers scroll through text—creates a synchronized reading experience. As Russell Goldenberg from The Pudding explains, this technique “works particularly well when users scroll through a single chart, triggering certain animations or changes as they progress.”\nProgressive disclosure reveals visualization elements sequentially as the narrative advances, preventing cognitive overload while building comprehension incrementally. The New York Times frequently employs this technique to introduce complex statistical concepts gradually.\nVisual hierarchy guides attention through strategic use of size, color, position, and detail. Alberto Cairo emphasizes that “visualizations are not made to be seen but to be read,” highlighting the importance of designing charts that direct the reader’s eye in a deliberate sequence.\nEffective visualization selection matches chart types to narrative purpose: comparisons use bar charts and dot plots; relationships employ scatter plots and networks; distributions utilize histograms and density plots. The most successful implementations consider both data structure and audience characteristics when selecting visualization formats.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study:What Have We Learned from the German Story?</span>"
    ]
  },
  {
    "objectID": "src/appx/case-study.html#making-data-meaningful-contextualizing-complexity-for-general-audiences",
    "href": "src/appx/case-study.html#making-data-meaningful-contextualizing-complexity-for-general-audiences",
    "title": "7  Case Study:What Have We Learned from the German Story?",
    "section": "8.3 Making data meaningful: Contextualizing complexity for general audiences",
    "text": "8.3 Making data meaningful: Contextualizing complexity for general audiences\nData journalists employ several techniques to make abstract statistics relatable:\nScale comparisons translate large numbers into concrete analogies (comparing plastic waste tonnage to landmark buildings, for instance). Personalization allows readers to see how data applies to their circumstances through interactive calculators or geographic filtering. Plain language replaces technical terminology with clear explanations without sacrificing accuracy.\nHuman interest integration weaves individual stories with aggregated data, giving faces to numbers. The Guardian’s coverage of the 2011 UK riots exemplified this approach by combining data analysis of riot locations with affected community interviews.\nAddressing the crucial “so what?” question requires explicit impact statements that directly explain why data matters. ProPublica’s “Surgeon Scorecard” project demonstrated how personalization makes healthcare quality data immediately relevant by allowing users to search for their local doctors and hospitals.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study:What Have We Learned from the German Story?</span>"
    ]
  },
  {
    "objectID": "src/appx/case-study.html#case-study-fivethirtyeights-german-election-analysis",
    "href": "src/appx/case-study.html#case-study-fivethirtyeights-german-election-analysis",
    "title": "7  Case Study:What Have We Learned from the German Story?",
    "section": "8.4 Case study: FiveThirtyEight’s German election analysis",
    "text": "8.4 Case study: FiveThirtyEight’s German election analysis\nThe 2017 article “Six Charts To Help Americans Understand The Upcoming German Election” exemplifies sophisticated data storytelling techniques through its structure, visualization integration, and contextual approach.\n\n8.4.1 Structural analysis\nThe article follows a clear logical progression from basic electoral mechanics to nuanced political analysis, organized around implicit questions an American reader might have. It begins with an accessible introduction acknowledging potential knowledge gaps: “You may have heard rumblings about a populist party poised to gain power in Germany’s election… or maybe you just heard that there’s an election coming up.”\nThe narrative moves from fundamental concepts (German electoral system) to increasingly complex contexts (party dynamics, regional differences) using a question-based framework that anticipates reader curiosity. This structure demonstrates the martini glass approach—guiding readers through essential background before opening to more detailed exploration.\n\n\n8.4.2 Visualization integration\nEach of the six visualizations serves distinct narrative functions that couldn’t be achieved through text alone:\nThe electoral system flow chart transforms a complex voting mechanism into a clear visual process, simplifying what might otherwise require paragraphs of explanation.\nThe ideological positioning chart places German parties on a left-right spectrum compared to US parties, creating an immediate reference point that helps Americans understand German politics through a familiar lens.\nThe party overview grid functions as a scannable reference guide that complements detailed textual descriptions of each party.\nThe regional comparison chart makes historical East-West divisions immediately visible through side-by-side data presentation.\nThe socioeconomic factors visualization reveals multidimensional patterns in party support that would be difficult to convey through text alone.\nThe AfD support timeline connects specific policy events to polling shifts, enabling readers to see cause-effect relationships in the rise of right-wing populism.\n\n\n8.4.3 Contextual techniques\nThe article excels at making foreign politics accessible through several contextual bridges:\nComparative framing consistently uses American political references as touchpoints: “These parties are somewhat analogous to the American Republican and Democratic parties, but both U.S. parties’ platforms are more conservative.”\nFamiliar parallels connect German dynamics to American experiences: “Like the division between the Union and the Confederacy in the United States, historical divisions between the former East and West still play a role.”\nControlled information density provides just enough detail to illuminate each point without overwhelming readers, balancing comprehensiveness with accessibility.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study:What Have We Learned from the German Story?</span>"
    ]
  },
  {
    "objectID": "src/appx/case-study.html#best-practices-in-data-journalism-storytelling",
    "href": "src/appx/case-study.html#best-practices-in-data-journalism-storytelling",
    "title": "7  Case Study:What Have We Learned from the German Story?",
    "section": "8.5 Best practices in data journalism storytelling",
    "text": "8.5 Best practices in data journalism storytelling\nEffective data journalism balances precision with accessibility through several key practices:\nTransparency in methodology builds credibility by documenting data sources, collection methods, and limitations. The Washington Post’s practice of publishing methodology details alongside major data projects demonstrates this commitment.\nBalancing author guidance with reader exploration requires judicious application of narrative structures. The martini glass approach works well for introducing unfamiliar topics to general audiences, while more knowledgeable readers might prefer drill-down structures that enable immediate independent exploration.\nVisual consistency maintains coherence through standardized color schemes, typography, and annotation styles. FiveThirtyEight’s minimalist visualization style employs a limited color palette with direct annotations rather than separate legends, creating visual harmony across different chart types.\nEthical presentation requires fair representation, privacy protection, and accessible design. This includes ensuring visualizations work for color-blind audiences, using inclusive language, and avoiding cherry-picked data that might support predetermined narratives.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study:What Have We Learned from the German Story?</span>"
    ]
  },
  {
    "objectID": "src/appx/case-study.html#conclusion",
    "href": "src/appx/case-study.html#conclusion",
    "title": "7  Case Study:What Have We Learned from the German Story?",
    "section": "8.6 Conclusion",
    "text": "8.6 Conclusion\nData journalism represents a sophisticated evolution in how information is communicated to general audiences. The most effective examples—like FiveThirtyEight’s German election analysis—transform complex datasets into compelling narratives through deliberate structural frameworks, strategic visualization integration, and contextual bridges that connect abstract statistics to reader experience.\nAs data increasingly shapes public discourse and policy decisions, the storytelling techniques identified in this analysis provide a framework for communicating complex information effectively. By balancing author guidance with reader exploration, maintaining visual coherence, and consistently connecting data to human impact, data journalists can illuminate important insights that might otherwise remain hidden in the numbers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study:What Have We Learned from the German Story?</span>"
    ]
  }
]